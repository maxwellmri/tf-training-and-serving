{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Training and Serving models with Tensforflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post will step you through the process for training and deploying a machine learning model for inference using tensorflow. You will be stepped through building and training a machine learning model using tensorflows `Estimator` object and then deployment of the model with the [Tensorflow Serving API](https://www.tensorflow.org/api_docs/serving/).\n",
    "\n",
    "Recent releases of Tensorflow saw the deprecation of some very useful experiment utilities such as `tf.contrib.learn.Experiment` and `tf.contrib.learn.learn_runner`. These utilities have now been replaced with the added function `tf.estimator.train_and_evaluate` to the `Estimator` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.Estimator\n",
    "The Estimator class is used for training and evaluation of tensorflow models. A model specified by a `model_fn` is wrapped in the `Estimator` object and returns the necessary operations for training, evaluation or prediction. You can read more about this class [here](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define your input function\n",
    "The input function tells the `Estimator` class how to get its training and evaluation data. This function must return `features` and `labels`. The output of the `input_fn` forms the input to the `model_fn` described in step 2 below. We define a `generate_input_function` that defines the training and evaluation datasets. We make use of Tensorflows `Dataset` API which is used to represent an input pipeline as a collection of elements and allows you to apply transformations such as batching, shuffling and mapping functions over the dataset. In this example we use the `TFRecordDataset` and define a mapping function `parse_function`. For the purposes of demonstrating the use of `tfrecords`, we convert the MNIST dataset into tfrecords (shown in the script `mnist_tfrecord_creator.py`). The `parse_function` has a features `dict` mapping the feature keys and takes as input a single serialised `Example`. This function returns a `dict` mapping feature keys to `Tensor` and `SparseTensor` values and is applied to each `Example` in the input tfrecords. We then have optional shuffling, set the batch size and create an `Interator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_fn(\n",
    "        filenames,\n",
    "        num_epochs=None,\n",
    "        shuffle=True,\n",
    "        batch_size=32):\n",
    "    def parse_function(example):\n",
    "        features = {\n",
    "            'image': tf.FixedLenFeature(\n",
    "                shape=[784],\n",
    "                dtype=tf.float32),\n",
    "            'label': tf.FixedLenFeature(\n",
    "                shape=[10],\n",
    "                dtype=tf.float32),\n",
    "            'id': tf.FixedLenFeature(\n",
    "                shape=[1],\n",
    "                dtype=tf.float32)\n",
    "        }\n",
    "\n",
    "        return tf.parse_single_example(example, features=features)\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(filenames).map(parse_function)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size * 10)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    \n",
    "    return features, features.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our training and evaluation input to the `Estimator` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = lambda: model.generate_input_fn(\n",
    "    args.train_files,\n",
    "    num_epochs=hparams.num_epochs,\n",
    "    batch_size=hparams.train_batch_size,\n",
    ")\n",
    "eval_input = lambda: model.generate_input_fn(\n",
    "    args.eval_files,\n",
    "    batch_size=hparams.eval_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define your model function\n",
    "For the purposes of this post we have taken the Deep MNIST model defined in this [tutorial](https://www.tensorflow.org/versions/r1.1/get_started/mnist/pros) and adjusted it for use with the `Estimator` class. Source code for the original Deep MNIST model used in the aforementioned tutorial is found [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist_deep.py).\n",
    "\n",
    "The first step for converting the original Deep MNIST model is to remove all placeholders as using the `Estimator` class removes the need for a feed dictionary, and thus placeholders. The keep probability for dropout used in the Deep MNSIT now becomes a hyperparameter defined in `hparams`. The `hparams` argument contains all hyperparameters and this argument is only passed along by the `Estimator` and is not inspected. Therefore, the structure of `hparams` is entirely up to you. The modified `deepnn` function from the original Deep MNIST tutorial is defined in `model.py`. The `model_fn` is defined below. This has two required inputs, `features` and `labels`, and returns an `EstimatorSpec` instance. `features` and `labels` are returned from your `input_function` discussed above. Optionally, the `mode` can also be passed to your `model_fn` which specifies if the model is in training, evaluation or prediction. See [ModeKeys](https://www.tensorflow.org/api_docs/python/tf/estimator/ModeKeys) for further information on this parameter.\n",
    "\n",
    "Depending on the mode, the required input arguments to the `EstimatorSpec` instance are different. See [here](https://www.tensorflow.org/api_docs/python/tf/estimator/EstimatorSpec) for more information. For training and evaluation specifically, we pass `loss`, `train_op` and `eval_metrics`. For prediction, we pass `export_outputs` which is used during serving and defines the output signatures to be exported to `SavedModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_fn(hparams):\n",
    "    def _model_fn(mode, features, labels):\n",
    "        image = features['image']\n",
    "        y_conv = deepnn(image, hparams.keep_prob)\n",
    "        prediction = tf.argmax(y_conv, 1)\n",
    "\n",
    "        if mode in [tf.estimator.ModeKeys.TRAIN, tf.estimator.ModeKeys.EVAL]:\n",
    "            loss = calculate_loss(labels, y_conv)\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(extra_update_ops):\n",
    "                train_op = tf.train.AdamOptimizer(hparams.learning_rate).minimize(\n",
    "                        loss,\n",
    "                        global_step=tf.train.get_global_step())\n",
    "            \n",
    "            eval_metrics = get_eval_metrics(labels, y_conv)\n",
    "            \n",
    "            # Add tensorboard summaries\n",
    "            tf.summary.scalar('loss', loss)\n",
    "            tf.summary.image('input', tf.reshape(image, [-1, 28, 28, 1]))\n",
    "            export_outputs = None\n",
    "        else:\n",
    "            loss = None\n",
    "            train_op = None\n",
    "            eval_metrics = None\n",
    "            export_outputs = {\n",
    "                'labels': tf.estimator.export.PredictOutput({\n",
    "                    'label': prediction,\n",
    "                    'id': features['id'],\n",
    "                })}\n",
    "\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode,\n",
    "            loss=loss,\n",
    "            train_op=train_op,\n",
    "            eval_metric_ops=eval_metrics,\n",
    "            predictions={\n",
    "                'label': tf.nn.softmax(tf.cast(prediction,tf.float32))},\n",
    "            export_outputs=export_outputs\n",
    "            )\n",
    "    return _model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wish to train your model across a multiple GPU's, a small change must be made to the optimization process used in the model function to facilitate computing and syncronising the correct gradients and associated weight updates on each card. Tensorflow provides the wrapper function `tf.contrib.estimator.TowerEstimator`, which can be used to transform a standard optimizor into one does this for us. If you don't have access to multiple GPU's, a single GPU or CPU is perfectly sufficient for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(hparams.learning_rate)\n",
    "tower_optimizer = tf.contrib.estimator.TowerOptimizer(optimizer)\n",
    "train_op = tower_optimiser.minimize(loss, global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about what this is doing under the hood just yet, we will explain that in a bit more detail once we get to training time!\n",
    "\n",
    "I should also point out that multi GPU training is most likely a little overkill for something as \"simple\" as MNIST. Whilst your training time will increase significantly, distributed training is something that will become a lot more useful when working with more complex problems and much larger networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define your serving function\n",
    "For the purposes of serving, we define a `serving_input_receiver_function`. This function expects a serialized `Example` and parses this according to the provided `feature_spec`. This function is also used for defining our model exports. Our serving input function is defined as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_serving_input_fn():\n",
    "    features = {\n",
    "        'image': tf.FixedLenFeature(\n",
    "            shape=[784],\n",
    "            dtype=tf.float32),\n",
    "        'label': tf.FixedLenFeature(\n",
    "            shape=[10],\n",
    "            dtype=tf.float32),\n",
    "        'id': tf.FixedLenFeature(\n",
    "            shape=[1],\n",
    "            dtype=tf.float32),\n",
    "    }\n",
    "    return tf.estimator.export.build_parsing_serving_input_receiver_fn(features)()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Set up your experiment\n",
    "In this section we describe how to setup your experiment using the above defined functions and how to create and use the `Estimator` class.\n",
    "Previously we mention `hparams`. `hparams` is defined by our input arguments to our experiment. For convenience we define `task.py` which defines all the input arguments to our model (including hyperparameters, training and evaluation files, output directory, etc.) and sets up our experiment. In here we map the input arguments for our experiment to the `Hparams` class and this object holds the input arguments as name-value pairs.\n",
    "\n",
    "#### Training and Evaluation Input Functions\n",
    "The `Estimator` requires us to define an `eval_spec` and `train_spec` instance. We define these below. The `EvalSpec` class requires the `eval_input` function defined above. In addition, we pass in a couple of optional arguments including:\n",
    "1. steps: the number is evaluation steps (default is 100)\n",
    "1. exporters: Iterable of `exporters` or a single one (default is None)\n",
    "1. throttle_secs: Time to wait between evaluations (default is 600)\n",
    "\n",
    "`exporter`s define the type of model export. For this example we use the `FinalExporter` class which performs a single model export at the end of training. There are other exporters available allowing you to set the frequency of exports and how many exports to keep.\n",
    "`throttle_secs` define the time between evaluations and evaluate the model at the lastest checkpoint. Note that if there are no new checkpoints, evaluation will not be performed. The default is 600 seconds but due to the short time for training completion for the MNIST model we set the time to 60 seconds to obtain regular evaluations. Unfortunately, the `Estimator` class does not allow you to set the frequency of evaluation according the the number of training steps completed. There is a current feature request to add this functionality [here](https://github.com/tensorflow/tensorflow/issues/17650#issuecomment-385097233) if you would to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exporters = []\n",
    "exporters.append(tf.estimator.FinalExporter(\n",
    "    'mnist', \n",
    "    model.example_serving_input_fn))\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "        eval_input, \n",
    "        steps=hparams.eval_steps, \n",
    "        exporters=exporters,\n",
    "        throttle_secs=60)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "        train_input,\n",
    "        max_steps=hparams.max_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Estimator\n",
    "Next we define our `Estimator` object that takes as input the `model_fn` described above. In addition, we pass in an optional parameter `config` to demonstrate modifying the default configuration parameters for the `Estimator`. For this object we only define the `model_dir` but other parameters that can be modified include:\n",
    "1. frequency in steps to save summaries\n",
    "1. number of checkpoints to keep\n",
    "\n",
    "For the full list please see [RunConfig](https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig).\n",
    "\n",
    "Our`Estimator` is defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "        model.generate_model_fn(hparams),\n",
    "        config=tf.estimator.RunConfig(\n",
    "            model_dir=hparams.job_dir,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are intending to train and evaluate your model on multiple GPU's (the model_fn from Step 2 must have been modified appropriately), another convenient wrapper function is provided by Tensorflow to help with this. `tf.contrib.estimator.replicate_model_fn` takes the model function returned by `generate_model_fn` and replicates the model across available GPU's. Replication pins identical copies of the model and its associated ops on each card (refered to generally as a Tower), whilst variables are pinned to the CPU and sharded and distributed to each Tower during the forward pass of the graph. On the backwards pass, losses are computed seperately on each Tower, and then aggregated on the CPU. The aggregated loss is then used by the `TowerOptimizer` we implemented in our model function to compute the gradients and update weights for each Tower, respectively. It can be helpful to analyse the visual graph of your model to fully understand this distributed backpropagation loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.Estimator(\n",
    "        tf.contrib.estimator.replicate_model_fn(model.generate_model_fn(hparams)),\n",
    "        config=tf.estimator.RunConfig(\n",
    "            model_dir=hparams.job_dir,\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Evaluation\n",
    "To perform training and evaluation we use the utility function `tf.estimator.train_and_evaluate`. We simply pass in our `estimator`, `train_spec` and `eval_spec` as shown below. This function performs training and evaluation to the given specifications defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.estimator.train_and_evaluate(\n",
    "    estimator, \n",
    "    train_spec, \n",
    "    eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Set up your model for serving\n",
    "Once your model has finished training and produced a `SavedModel` export, you can now set up your model for serving.\n",
    "#### Check your saved model exports\n",
    "This next step is more of a sanity check to confirm the input and output tensors for our model and the signature name. We defined these earlier in our `export_outputs` in the `EstimatorSpec` and in the `example_seving_input_function`. These definitions are needed in order to query our servable. Tensorflow provides a [SavedModel CLI](https://www.tensorflow.org/versions/r1.2/programmers_guide/saved_model_cli) for inspecting `SavedModels`. To show all available information we run the following command (note `job-dir` was defined previously for our `Estimator`. This can be a google cloud bucket or a local directory):\n",
    "\n",
    "`saved_model_cli show --dir <job-dir>/logs/Experiment#/export/mnist/<timestamp> --all`\n",
    "\n",
    "The ouput from the above is:\n",
    "\n",
    "```\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "signature_def['labels']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['examples'] tensor_info:\n",
    "        dtype: DT_STRING\n",
    "        shape: (-1)\n",
    "        name: input_example_tensor:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['id'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 1)\n",
    "        name: ParseExample/ParseExample:0\n",
    "    outputs['label'] tensor_info:\n",
    "        dtype: DT_INT64\n",
    "        shape: (-1)\n",
    "        name: ArgMax:0\n",
    "  Method name is: tensorflow/serving/predict\n",
    "\n",
    "signature_def['serving_default']:\n",
    "  The given SavedModel SignatureDef contains the following input(s):\n",
    "    inputs['examples'] tensor_info:\n",
    "        dtype: DT_STRING\n",
    "        shape: (-1)\n",
    "        name: input_example_tensor:0\n",
    "  The given SavedModel SignatureDef contains the following output(s):\n",
    "    outputs['id'] tensor_info:\n",
    "        dtype: DT_FLOAT\n",
    "        shape: (-1, 1)\n",
    "        name: ParseExample/ParseExample:0\n",
    "    outputs['label'] tensor_info:\n",
    "        dtype: DT_INT64\n",
    "        shape: (-1)\n",
    "        name: ArgMax:0\n",
    "  Method name is: tensorflow/serving/predict\n",
    "\n",
    "```\n",
    "\n",
    "#### Build your docker image\n",
    "\n",
    "We will run our servable inside a docker container. For Docker installation instructions and more information please visit: https://docs.docker.com/engine/installation/.\n",
    "\n",
    "We start by defining the required dependenies and package installation instructions in a Dockerfile. The dependencies for tensorflow serving are listed here https://github.com/tensorflow/serving/blob/master/tensorflow_serving/g3doc/setup.md. Please refer to the Dockerfile [hereURLPLEASE]().\n",
    "I will draw your attention to the last line in the Dockerfile:\n",
    "```\n",
    "CMD /usr/bin/tensorflow_model_server --port=<container-port> --model_name=<model_name> --model_base_path=/exports\n",
    "```\n",
    "This is the line of code that runs the servable inside the docker container. The `model_name` you will need to remember for the grpc client we will setup later to query the servable. The `--port` argument is the port exposed in the container.\n",
    "\n",
    "Next we build our docker image with the following:\n",
    "\n",
    "`docker build -t <image_name> .`\n",
    "\n",
    "To run the servable in the container we need to copy the model exports into the container or mount them when we run it. Tensorflow serving supports versioning and we need to make sure the model exports are saved in numbered directories. Your directory structure should look something like this (assuming Version 1):\n",
    "```\n",
    "exports/1/variables/...\n",
    "exprots/1/saved_model.pb file\n",
    "```\n",
    "To run the docker container and mount the model exports we run the following command:\n",
    "\n",
    "`docker run -it -p <external-port>:<container-port> -v <model-exports-directory>:/exports -e MODEL_NAME=<model_name> --name <container-name> <image-name>`\n",
    "\n",
    "Note that `/exports` is the `model_base_path` that we defined in our Dockerfile. The above command runs the container in the foreground. If you wish to run in the backgrund replace `-it` with `-d`. If running in the background, you can view the logs with:\n",
    "`docker logs <container-name>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up gRPC client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow serving uses GRPC protocol and we need to create a client to issue inference requests. First, we need to install python GRPC libraries:\n",
    "```\n",
    "pip install grpcio grpcio-tools \n",
    "```\n",
    "The noteworthy parts of creating your client are as follows:\n",
    "\n",
    "1. Firstly, we create our prediction request object:\n",
    "\n",
    "    ```\n",
    "    request = predict_pb2.PredictRequest() \n",
    "    ```\n",
    "2. Secondly, we initialise the prediction request object with the details of our model. The details required are:\n",
    "    * Model name\n",
    "    * Signature name\n",
    "\n",
    "    The model name we defined when we started our model server. The signature name is as defined in our `export_outputs` from step 2. Alternatively we can use the `DEFAULT_SERVING_SIGNATURE_DEF_KEY` defined [here](https://www.tensorflow.org/api_docs/python/tf/saved_model/signature_constants).\n",
    "\n",
    "    ```\n",
    "     request.model_spec.name = model_name\n",
    "     request.model_spec.signature_name = signature_name\n",
    "    ```\n",
    "3. Lastly we define our input data for inference and issue the request.\n",
    "    The input data for inference needs to be a `TensorProto`. We get this by first creating `tf.train.Example`s and serialising these to a string. Similar to what we did in previous steps we define our features and create examples from the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {\n",
    "   \"image\":\n",
    "   tf.train.Feature(\n",
    "       float_list=tf.train.FloatList(value=image.flatten().tolist())),\n",
    "   \"label\":\n",
    "   tf.train.Feature(\n",
    "       float_list=tf.train.FloatList(value=label.flatten().tolist())),\n",
    "   \"id\":\n",
    "   tf.train.Feature(\n",
    "       float_list=tf.train.FloatList(value=[_id])),\n",
    "}\n",
    "features = tf.train.Features(feature=features)\n",
    "example = tf.train.Example(features=features)\n",
    "example_serialized = example.SerializeToString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These serialised examples become the input to our request:\n",
    "```\n",
    "request.inputs['examples'].CopyFrom(tf.contrib.util.make_tensor_proto([example_serialized], dtype=tf.string))\n",
    "```\n",
    "We call prediction on our servable with:\n",
    "```\n",
    "result = self.stub.Predict(request, timeout) \n",
    "```\n",
    "For convenience we create a ServableClient class in `servable_client.py` which implements the above.\n",
    "\n",
    "Following is a complete example of creating the client, defining the input data, querying the servable and viewing the results. The below can also be found in `mnist_query.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/data2/conda/envs/py27/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from servable_client import ServableClient, create_features\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "val = mnist.validation.next_batch(10)\n",
    "features = []\n",
    "i = 0\n",
    "for img, label in zip(val[0], val[1]):\n",
    "    features.append(create_features(img, label, i))\n",
    "    i=+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "servable_client = ServableClient(\"10.10.0.10\", \"mnist-serving\", port=9001)\n",
    "results = []\n",
    "for feature in features:\n",
    "    result = servable_client.do_inference(feature)\n",
    "    label = result.outputs['label'].int64_val[0]\n",
    "    results.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Predicted: 9', 'Label: 9')\n",
      "('Predicted: 8', 'Label: 8')\n",
      "('Predicted: 8', 'Label: 8')\n",
      "('Predicted: 0', 'Label: 0')\n",
      "('Predicted: 1', 'Label: 1')\n",
      "('Predicted: 8', 'Label: 8')\n",
      "('Predicted: 1', 'Label: 1')\n",
      "('Predicted: 6', 'Label: 6')\n",
      "('Predicted: 5', 'Label: 5')\n",
      "('Predicted: 0', 'Label: 0')\n"
     ]
    }
   ],
   "source": [
    "for label, pred in zip(val[1], results):\n",
    "    print('Predicted: %d' % pred, 'Label: %d' % np.argmax(label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
